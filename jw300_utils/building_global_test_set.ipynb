{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "building_global_test_set.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kabongosalomon/RDC-Mobongoli/blob/main/jw300_utils/building_global_test_set.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wH8AJ3OAVBpF",
        "outputId": "8de80ca8-47a3-4986-eed5-ea0413dd3885"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLIkGn6RQ5yg"
      },
      "source": [
        "### Setting Up the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_uBo3H6Q5yr"
      },
      "source": [
        "Downloading the global test set is simple,\n",
        "we need to set english and your target as source and target language, then we find the intersection of the english test set with the target corpus after that we get the corresponding target sentences from the target corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rdvcs5aRVbf"
      },
      "source": [
        "%%capture\n",
        "!pip install opustools-pkg"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FG5qoTCkdyCy"
      },
      "source": [
        "# SET THE LANGUAGE CODE and other variables.\n",
        "\n",
        "You need to change the value below for your language!\n",
        "\n",
        "The language codes from the [JW300 corpus website](https://object.pouta.csc.fi/OPUS-JW300/v1/languages.json) are: \n",
        "```\n",
        "{\n",
        "    \"language\": \"French - Français\",\n",
        "    \"language_en\": \"French\",\n",
        "    \"language_native\": \"Français\",\n",
        "    \"language_short\": \"fr\",\n",
        "    \"url\": \"https://wol.jw.org/fr/wol/pref/r30/lp-f?newrsconf=r30&newlib=lp-f&url=\"\n",
        "}, \n",
        "\n",
        "{\n",
        "    \"language\": \"Lingala - Lingala\",\n",
        "    \"language_en\": \"Lingala\",\n",
        "    \"language_native\": \"Lingala\",\n",
        "    \"language_short\": \"ln\",\n",
        "    \"url\": \"https://wol.jw.org/ln/wol/pref/r126/lp-li?newrsconf=r126&newlib=lp-li&url=\"\n",
        "},\n",
        "\n",
        "{\n",
        "    \"language\": \"Tshiluba - Tshiluba\",\n",
        "    \"language_en\": \"Tshiluba\",\n",
        "    \"language_native\": \"Tshiluba\",\n",
        "    \"language_short\": \"lua\",\n",
        "    \"url\": \"https://wol.jw.org/lua/wol/pref/r477/lp-sh?newrsconf=r477&newlib=lp-sh&url=\"\n",
        "},\n",
        "\n",
        "{\n",
        "    \"language\": \"Kikongo - Kikongo\",\n",
        "    \"language_en\": \"Kikongo\",\n",
        "    \"language_native\": \"Kikongo\",\n",
        "    \"language_short\": \"kwy\",\n",
        "    \"url\": \"https://wol.jw.org/kwy/wol/pref/r128/lp-kg?newrsconf=r128&newlib=lp-kg&url=\"\n",
        "},\n",
        "\n",
        "{\n",
        "    \"language\": \"Swahili (Congo) - Kiswahili (Congo)\",\n",
        "    \"language_en\": \"Swahili (Congo)\",\n",
        "    \"language_native\": \"Kiswahili (Congo)\",\n",
        "    \"language_short\": \"swc\",\n",
        "    \"url\": \"https://wol.jw.org/swc/wol/pref/r143/lp-zs?newrsconf=r143&newlib=lp-zs&url=\"\n",
        "  },\n",
        "```\n",
        "Already-created test sets: https://raw.githubusercontent.com/ai-drc/RDC-Mobongoli/main/jw300_utils/test/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhNiQZplQ5yt"
      },
      "source": [
        "import os\n",
        "source_language = \"fr\"\n",
        "target_language = \"ln\" # TODO: CHANGE THIS TO YOUR LANGUAGE! \"ha\" is hausa. See the language codes at https://opus.nlpl.eu/JW300.php\n",
        "lc = False  # If True, lowercase the data.\n",
        "seed = 42  # Random seed for shuffling.|\n",
        "tag = \"baseline\" # Give a unique name to your folder - this is to ensure you don't rewrite any models you've already submitted\n",
        "\n",
        "os.environ[\"src\"] = source_language # Sets them in bash as well, since we often use bash scripts\n",
        "os.environ[\"tgt\"] = target_language\n",
        "os.environ[\"tag\"] = tag\n",
        "\n",
        "# No need to use gdrive since we are training on gcp\n",
        "!mkdir -p \"$src-$tgt-$tag\"\n",
        "os.environ[\"gdrive_path\"] = \"%s-%s-%s\" % (source_language, target_language, tag) # saving directly on the vm"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RExQ_q-vQ5yy",
        "outputId": "93afdc07-f325-4eef-a193-2035d59b966e"
      },
      "source": [
        "!echo $gdrive_path"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fr-ln-baseline\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6C760IqtQ5y0"
      },
      "source": [
        "#### Downloading the corpus data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1jEhHm8Q5y1"
      },
      "source": [
        "for precaution , am removing the old data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQBYlyBSQ5y3"
      },
      "source": [
        "!rm -f w300.$src jw300.$tgt JW300_latest_xml_$src-$tgt.xml.gz JW300_latest_xml_$src-$tgt.xml JW300_latest_xml_$src.zip  JW300_latest_xml_$tgt.zip test.fr-any.fr"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2bhVsAEQ5y4",
        "scrolled": true,
        "outputId": "41a37f7e-dbc6-4fce-c059-2d1bf7915222"
      },
      "source": [
        "# Downloading our corpus\n",
        "! opus_read -d JW300 -s $src -t $tgt -wm moses -w jw300.$src jw300.$tgt -q\n",
        "\n",
        "# extract the corpus file\n",
        "! gunzip JW300_latest_xml_$src-$tgt.xml.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Alignment file /proj/nlpl/data/OPUS/JW300/latest/xml/fr-ln.xml.gz not found. The following files are available for downloading:\n",
            "\n",
            "   6 MB https://object.pouta.csc.fi/OPUS-JW300/v1b/xml/fr-ln.xml.gz\n",
            " 278 MB https://object.pouta.csc.fi/OPUS-JW300/v1b/xml/fr.zip\n",
            "  60 MB https://object.pouta.csc.fi/OPUS-JW300/v1b/xml/ln.zip\n",
            "\n",
            " 345 MB Total size\n",
            "./JW300_latest_xml_fr-ln.xml.gz ... 100% of 6 MB\n",
            "./JW300_latest_xml_fr.zip ... 100% of 278 MB\n",
            "./JW300_latest_xml_ln.zip ... 100% of 60 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdJrG4SmQ5y7"
      },
      "source": [
        "! wget https://raw.githubusercontent.com/ai-drc/RDC-Mobongoli/main/jw300_utils/test/test.$src-any.$src\n",
        "\n",
        "  \n",
        "# And the specific test set for this language pair.\n",
        "os.environ[\"trg\"] = target_language \n",
        "os.environ[\"src\"] = source_language "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XLatU0iQ5y8"
      },
      "source": [
        "# Read the test data to filter from train and dev splits.\n",
        "# Store english portion in set for quick filtering checks.\n",
        "en_test_sents = set()\n",
        "filter_test_sents = f\"test.{source_language}-any.{source_language}\"\n",
        "j = 0\n",
        "with open(filter_test_sents) as f:\n",
        "  for line in f:\n",
        "    en_test_sents.add(line.strip())\n",
        "    j += 1\n",
        "print('Loaded {} global test sentences to filter from the training/dev data.'.format(j))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydypbA7LQ5y-"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTsD-Ta1Q5zD"
      },
      "source": [
        "#### Building the corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8I8fMRRrQ5zF"
      },
      "source": [
        "In the 2 cells below you can check if the 2 datasets are aligned. Even if you don't speak the language you can get a sense, especially with similar words, punctuation, and so forth."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHpOmS0xQ5zF",
        "scrolled": true
      },
      "source": [
        "! head -5 jw300.$src"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgPwLbD8Q5zG"
      },
      "source": [
        "! head -5 jw300.$tgt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlJP5HzhQ5zJ"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# TMX file to dataframe\n",
        "source_file = 'jw300.' + source_language  ## source language is english\n",
        "target_file = 'jw300.' + target_language ## Target is whatever you set. For our example it was ha, so jw300.ha\n",
        "target_test = {}\n",
        "source = []\n",
        "target = []\n",
        "english_sentences_in_global_test_set = {}  # Collect the line numbers of the source portion to skip the same lines for the target portion.\n",
        "with open(source_file) as src_f:\n",
        "    for i, line in enumerate(src_f):\n",
        "        # Skip sentences that are contained in the test set and add them into the new frencg test\n",
        "        if line.strip() not in en_test_sents:\n",
        "            source.append(line.strip())\n",
        "        else:\n",
        "            # Here is the intersection with the global test set\n",
        "            english_sentences_in_global_test_set[i] = line.strip()           \n",
        "with open(target_file) as f:\n",
        "    for j, line in enumerate(f):\n",
        "        # Only add to corpus if corresponding source was not skipped.\n",
        "        if j not in english_sentences_in_global_test_set.keys():\n",
        "            target.append(line.strip())\n",
        "        else:\n",
        "            #Collecting the aligned test sentences\n",
        "            target_test[j] = line.strip()\n",
        "    \n",
        "print('Loaded data and skipped {}/{} lines since contained in test set.'.format(len(english_sentences_in_global_test_set.keys()), i))\n",
        "    \n",
        "df = pd.DataFrame(zip(source, target), columns=['source_sentence', 'target_sentence'])\n",
        "# if you get TypeError: data argument can't be an iterator is because of your zip version run this below\n",
        "#df = pd.DataFrame(list(zip(source, target)), columns=['source_sentence', 'target_sentence'])\n",
        "df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krpoFeY_TtrG"
      },
      "source": [
        "## Check a random item!\n",
        "Let's pick one of the keys in the dictionary at random and check it. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGBDbByaQ5zL",
        "scrolled": true
      },
      "source": [
        "import random\n",
        "keys_in_target_test = list(target_test.keys())\n",
        "print(type(keys_in_target_test))\n",
        "random_key = random.choice(keys_in_target_test)\n",
        "print(f\"The random key we picked was {random_key}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0rjaSMyUzMt"
      },
      "source": [
        "target_test[random_key]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_H63DagnQ5zM"
      },
      "source": [
        "english_sentences_in_global_test_set[random_key]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pwuc4IlEe1TR"
      },
      "source": [
        "Do the two look like they line up? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5mtDXEoe9NC"
      },
      "source": [
        "## Check several rows at the tail end\n",
        "\n",
        "Let's get a sample from the end of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uD7ht8dKQ5zN"
      },
      "source": [
        "target_test_set = pd.DataFrame(zip(target_test.values(), english_sentences_in_global_test_set.values()), columns=[f'{target_language}_equivalent', f'{source_language}_equivalent'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QclpZObEQ5zN"
      },
      "source": [
        "target_test_set = target_test_set.reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqPm3EDTQ5zN"
      },
      "source": [
        "target_test_set = target_test_set.set_index(\"index\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnXQ31qOQ5zO"
      },
      "source": [
        "target_test_set.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-7KKhEcQ5zO"
      },
      "source": [
        "Removing duplicates from english and target set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0sCRCX2Q5zO"
      },
      "source": [
        "target_test_set = target_test_set.drop_duplicates(subset=f'{target_language}_equivalent')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6biqqTgUQ5zP"
      },
      "source": [
        "target_test_set = target_test_set.drop_duplicates(subset=f'{source_language}_equivalent')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrqizJY3Q5zQ"
      },
      "source": [
        "target_test_set.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJDI9P-LQ5zS"
      },
      "source": [
        "target_test_set.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyWb1BPsQ5zT"
      },
      "source": [
        "target_test_set.loc[~target_test_set[f'{source_language}_equivalent'].isin(en_test_sents)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFBOZerTjxSI"
      },
      "source": [
        "## Write out target-language test set file\n",
        "In our example, we should have `test.ln-any.ln`, but it will be different for you if you picked a different code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6zxiOfdVab2"
      },
      "source": [
        "target_test_filename = f\"test.{target_language}-any.{target_language}\"\n",
        "print(target_test_filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LXXXfwjfZQq"
      },
      "source": [
        "## Write out English-language test set file\n",
        "In our example, we should have `test.fr-ln.fr`, but it will be different for you if you picked a different code.\n",
        "\n",
        "**Make sure the data lines up in the two files!**\n",
        "The first line of each file should be translations of each other.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpFWMOhdQ5zU"
      },
      "source": [
        "\n",
        "with open(target_test_filename, \"w\") as test_tgt_any_tgt:\n",
        "    test_tgt_any_tgt.write(\"\\n\".join(target_test_set[f'{target_language}_equivalent']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnZUf97gQ5zV"
      },
      "source": [
        "!head -5 test.$tgt-any.$tgt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZ93jNULQ5zV"
      },
      "source": [
        "source_test_filename = f\"test.{source_language}-{target_language}.{source_language}\"\n",
        "print(f\"saving english aligned sentences to {source_test_filename}\")\n",
        "with open(source_test_filename, \"w\") as test_src_tgt_src:\n",
        "    test_src_tgt_src.write(\"\\n\".join(target_test_set[f'{source_language}_equivalent']))\n",
        "!ls -al"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKW2y01hbOyc"
      },
      "source": [
        "!head -5 test.$src-$tgt.$src"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsGlxBIxekZ8"
      },
      "source": [
        "source_test_filename = f\"test.{source_language}-{target_language}.{target_language}\"\n",
        "print(f\"saving english aligned sentences to {source_test_filename}\")\n",
        "with open(source_test_filename, \"w\") as test_en_tgt_tgt:\n",
        "    test_en_tgt_tgt.write(\"\\n\".join(target_test_set[f'{target_language}_equivalent']))\n",
        "!ls -al"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzNlzeszd5ha"
      },
      "source": [
        "!head -5 test.$src-$tgt.$tgt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kr79AnMtfxyQ"
      },
      "source": [
        "## One last check to see if the two files are aligned\n",
        "\n",
        "Let's just get one more sample! Let's take from the end this time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvmVNumtf3sC"
      },
      "source": [
        "!echo \"test.$src-$tgt.$src\"\n",
        "!tail -5 test.$src-$tgt.$src\n",
        "!echo\n",
        "!echo \"**********************\"\n",
        "!echo \"test.$src-$tgt.$tgt\"\n",
        "!tail -5 test.$src-$tgt.$tgt\n",
        "!echo\n",
        "!echo \"**********************\"\n",
        "!echo \"test.$tgt-any.$tgt\"\n",
        "!echo \"**********************\"\n",
        "!tail -5 test.$tgt-any.$tgt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2miJypOj0rg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}